# canal配置 单机
canal.server.ip=hadoop001
canal.server.port=11111
canal.server.destination=example
canal.server.username=root
canal.server.password=root
canal.subscribe.filter=ekko_shop.*

# zookeeper配置
zookeeper.server.ip=hadoop001:2181,hadoop002:2181,hadoop003:2181

# kafka配置 集群地址
kafka.bootstrap_servers_config=hadoop001:9092,hadoop002:9092,hadoop003:9092
#批次发送数据的大小
kafka.batch_size_config=1024
# 1: 表示leader节点写入成功,就返回,假设leader节点写入成功后没有来得及同步,宕机了,数据会丢失 [线程阻塞]
# 0: 异步操作,不管有没有写入成功,都返回,存在数据丢失的情况 [效率高]
# -1:当leader节点写入成功,.同时从节点同步成功以后才会返回,可以保证数据的不丢失 [线程阻塞]
kafka.acks=all
#重试次数
kafka.retries=0
kafka.client_id_config=ekko_shop_canal_click
#kafka的key序列化
kafka.key_serializer_class_config=org.apache.kafka.common.serialization.StringSerializer
#kafka的value序列化 [自定义开发]
kafka.value_serializer_class_config=cn.ekko.canal.protobuf.ProtoBufSerializer
#数据写入的kafka topic 中
kafka.topic=ods_ekko_shop_mysql